{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from time import sleep\n",
    "import itertools\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from zipfile import ZipFile\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".envrc\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "path = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/recent/\"\n",
    "path = Path(\"/final/main\")\n",
    "print(path.name == \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 575 files for download\n",
      "575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URLS = {\n",
    "#     \"hist_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\",\n",
    "#     \"recent_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/recent/\",\n",
    "# }\n",
    "\n",
    "def get_file_links(url: str, category: str) -> list:\n",
    "        \"\"\"Get urls of files\"\"\"\n",
    "        tmp_url = url + category \n",
    "        page = requests.get(tmp_url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        \n",
    "\n",
    "        links = soup.find_all(\"a\", href=re.compile(\".pdf$|.txt$|.zip$\"))\n",
    "        print(\"found {} files for download\".format(len(links)))\n",
    "        \n",
    "        return [[link, category] for link in links]\n",
    "\n",
    "\n",
    "def download_files(link: list) -> None:\n",
    "    link, category = link \n",
    "    print(link, category)\n",
    "\n",
    "    save_path = Path(f\"./data/{category}/download\")\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    file_path = save_path / link[\"href\"]\n",
    "    mode = \"w+b\" if \"pdf\" or \"zip\" in link[\"href\"] else \"w+\"\n",
    "    file_url = url + link[\"href\"]\n",
    "    if not file_path.is_file():\n",
    "        with requests.get(file_url) as response:\n",
    "            with open(str(file_path), mode) as file:\n",
    "                file.write(response.content)\n",
    "                sleep(0.5)\n",
    "    \n",
    "    print(\"download finished\")\n",
    "    # TODO: add error handler for timeout with too many requests\n",
    "url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/\"\n",
    "category = \"recent\"\n",
    "links = get_file_links(url, category)\n",
    "# with ThreadPool(5) as pool:\n",
    "#     for count, result in enumerate(pool.imap_unordered(download_files, links)):\n",
    "#         print(f\"downloads finished: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in Path(\"data/\").glob(\"*_data\"):\n",
    "        source_path = path / \"download\"\n",
    "        source_path.mkdir(parents=True, exist_ok=True)\n",
    "        target_data_path = path / \"tables\"\n",
    "        target_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        target_metadata_path = path / \"metadata\"\n",
    "        target_metadata_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file in source_path.glob(\"*.zip\"):\n",
    "            with ZipFile(file, \"r\") as zip:\n",
    "                fname_data_lst = [\n",
    "                    fname for fname in zip.namelist() if \"produkt_klima_tag\" in fname\n",
    "                ]\n",
    "                fname_metadata_lst = [\n",
    "                    fname for fname in zip.namelist() if \"Metadaten\" in fname\n",
    "                ]\n",
    "                for fname in fname_data_lst:\n",
    "                    zip.extract(fname, target_path)\n",
    "                for fname in fname_metadata_lst:\n",
    "                    zip.extract(fname, target_path)\n",
    "\n",
    "        files_to_rmv = target_metadata_path.glob(\"*.html\")\n",
    "        for file in files_to_rmv:\n",
    "            file.unlink()\n",
    "\n",
    "        print(\"finished unzipping to folder {}\".format(path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes= {\n",
    "    'STATIONS_ID': \"string\", \n",
    "    'QN_3': \"UInt8\", \n",
    "    'QN_4': \"UInt8\", \n",
    "    ' RSK': \"Float32\", \n",
    "    'RSKF': \"UInt8\", \n",
    "    'SHK_TAG': \"UInt8\",\n",
    "    '  NM': \"string\", \n",
    "    ' TMK': \"Float32\", \n",
    "    ' UPM': \"Float32\", \n",
    "    ' TXK': \"Float32\", \n",
    "    ' TNK': \"Float32\", \n",
    "    ' TGK': \"Float32\",\n",
    "}\n",
    "\n",
    "\n",
    "df_new_lst = [pd.read_table(\n",
    "        file, \n",
    "        sep=\";\",\n",
    "        usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "        'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "        ' TXK', ' TNK', ' TGK'],\n",
    "        dtype_backend=\"pyarrow\",\n",
    "        dtype=dtypes,\n",
    "        na_values=None,\n",
    "    ) for file in Path(\"./data/recent_data/extracts\").glob(\"*.txt\")]\n",
    "\n",
    "df_hist_lst = [pd.read_table(\n",
    "        file, \n",
    "        sep=\";\",\n",
    "        usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "        'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "        ' TXK', ' TNK', ' TGK'],\n",
    "        dtype_backend=\"pyarrow\",\n",
    "        dtype=dtypes,\n",
    "        na_values=None,\n",
    "    ) for file in Path(\"./data/hist_data/extracts\").glob(\"*.txt\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "\n",
    "# df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\", nonexistent=\"NaT\")\n",
    "# df.columns = df.columns.str.replace(' ', '')\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 4, 3, 0, 0, tzinfo=<DstTzInfo 'Europe/Brussels' CEST+2:00:00 DST>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_parquet(\"./data/parquet/data.parquet\")\n",
    "# df = pd.read_parquet(\"./data/final/main/\", dtype_backend=\"pyarrow\")\n",
    "df.mess_datum.max()\n",
    "\n",
    "# df[\"year_month\"] = df[\"mess_datum\"].dt.strftime(\"%Y-%m\")\n",
    "# df.to_parquet(\"./data/parquet/main/\", partition_cols=[\"year_month\"], compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes = {\n",
    "#     'Stations_id': \"string\",\n",
    "#     'Stationshoehe': \"Float32\",\n",
    "#     'Geogr.Breite': \"Float32\",\n",
    "#     'Geogr.Laenge': \"Float32\",\n",
    "#     'von_datum': \"string\",\n",
    "#     'bis_datum': \"string\",\n",
    "#     'Stationsname': \"string\",\n",
    "# }\n",
    "\n",
    "# df_new_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype=dtypes,\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "# df_hist_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype=dtypes,\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/hist_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "# df.Stations_id = df.Stations_id.str.replace(' ', '')\n",
    "# df = df.replace(-999, None).astype(dtypes)\n",
    "# df[\"von_datum\"] = pd.to_datetime(df[\"von_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df[\"bis_datum\"] = pd.to_datetime(df[\"bis_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df.columns = df.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_geo.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/metadata_geo.parquet\", dtype_backend=\"pyarrow\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_lst = []\n",
    "for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Stationsname_Betreibername*\"):\n",
    "    with open(file, \"r\", encoding=\"latin1\") as txt_file:\n",
    "                    text = txt_file.read()\n",
    "                    table_with_footer = text.split(\"\\n\\nStations_ID;Betreibername;Von_Datum;Bis_Datum\\n \")[1]\n",
    "                    df_new_lst += [line.split(\";\") for line in table_with_footer.splitlines()[:-1]]\n",
    "\n",
    "df = pd.DataFrame(df_new_lst, columns=[\"stations_id\", \"betreibername\", \"betrieb_von_datum\", \"betrieb_bis_datum\"])    \n",
    "df.stations_id = df.stations_id.str.replace(' ', '')\n",
    "df[\"betrieb_von_datum\"] = pd.to_datetime(df[\"betrieb_von_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "df[\"betrieb_bis_datum\"] = pd.to_datetime(df[\"betrieb_bis_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stations_id                                      string[pyarrow]\n",
       "mess_datum            timestamp[us, tz=Europe/Brussels][pyarrow]\n",
       "qn_3                                              uint8[pyarrow]\n",
       "qn_4                                              uint8[pyarrow]\n",
       "rsk                                               float[pyarrow]\n",
       "rskf                                              uint8[pyarrow]\n",
       "shk_tag                                           uint8[pyarrow]\n",
       "nm                                               string[pyarrow]\n",
       "tmk                                               float[pyarrow]\n",
       "upm                                               float[pyarrow]\n",
       "txk                                               float[pyarrow]\n",
       "tnk                                               float[pyarrow]\n",
       "tgk                                               float[pyarrow]\n",
       "year           dictionary<values=int32, indices=int32, ordere...\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_operator.parquet\")\n",
    "# df = pd.read_parquet(\"./data/final/main/\", dtype_backend=\"pyarrow\")\n",
    "df."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.types as T\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# spark.conf.set('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'CORRECTED')\n",
    "\n",
    "# schema = T.StructType([\n",
    "#     T.StructField('STATIONS_ID', T.StringType(), False),\n",
    "#     T.StructField('MESS_DATUM', T.TimestampType(), True),\n",
    "#     T.StructField('QN_3', T.ShortType(), True),\n",
    "#     T.StructField('QN_4', T.ShortType(), True),\n",
    "#     T.StructField('RSK', T.DoubleType(), True),\n",
    "#     T.StructField('RSKF', T.ShortType(), True),\n",
    "#     T.StructField('SHK_TAG', T.IntegerType(), True),\n",
    "#     T.StructField('NM', T.StringType(), True),\n",
    "#     T.StructField('TMK', T.DoubleType(), True),\n",
    "#     T.StructField('UPM', T.DoubleType(), True),\n",
    "#     T.StructField('TXK', T.DoubleType(), True),\n",
    "#     T.StructField('TNK', T.DoubleType(), True),\n",
    "#     T.StructField('TGK', T.DoubleType(), True),\n",
    "# ])\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName('SparkNotebook') \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path(\"./data/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .parquet(str(data_path))\n",
    "\n",
    "# df.createOrReplaceTempView(\"temps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT distinct(extract(year from MESS_DATUM)) from temps\n",
    "# where date(MESS_DATUM) >= \"1900-01-01\"\n",
    "# order by 1 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT STATIONS_ID,extract(year from MESS_DATUM), max(TXK) as max_temp_2m from temps\n",
    "# group by 1,2\n",
    "# order by 3 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week7--DKIWNv2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fb13844f58101a74e10bb1f8ffe6c9a5d7ea85b5210f5bc43be278327a5ddab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
