{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from time import sleep\n",
    "import itertools\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from zipfile import ZipFile\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".envrc\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "path = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/recent/\"\n",
    "path = Path(\"/final/main\")\n",
    "print(path.name == \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 575 files for download\n",
      "575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URLS = {\n",
    "#     \"hist_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\",\n",
    "#     \"recent_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/recent/\",\n",
    "# }\n",
    "\n",
    "def get_file_links(url: str, category: str) -> list:\n",
    "        \"\"\"Get urls of files\"\"\"\n",
    "        tmp_url = url + category \n",
    "        page = requests.get(tmp_url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        \n",
    "\n",
    "        links = soup.find_all(\"a\", href=re.compile(\".pdf$|.txt$|.zip$\"))\n",
    "        print(\"found {} files for download\".format(len(links)))\n",
    "        \n",
    "        return [[link, category] for link in links]\n",
    "\n",
    "\n",
    "def download_files(link: list) -> None:\n",
    "    link, category = link \n",
    "    print(link, category)\n",
    "\n",
    "    save_path = Path(f\"./data/{category}/download\")\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    file_path = save_path / link[\"href\"]\n",
    "    mode = \"w+b\" if \"pdf\" or \"zip\" in link[\"href\"] else \"w+\"\n",
    "    file_url = url + link[\"href\"]\n",
    "    if not file_path.is_file():\n",
    "        with requests.get(file_url) as response:\n",
    "            with open(str(file_path), mode) as file:\n",
    "                file.write(response.content)\n",
    "                sleep(0.5)\n",
    "    \n",
    "    print(\"download finished\")\n",
    "    # TODO: add error handler for timeout with too many requests\n",
    "url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/\"\n",
    "category = \"recent\"\n",
    "links = get_file_links(url, category)\n",
    "# with ThreadPool(5) as pool:\n",
    "#     for count, result in enumerate(pool.imap_unordered(download_files, links)):\n",
    "#         print(f\"downloads finished: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in Path(\"data/\").glob(\"*_data\"):\n",
    "        source_path = path / \"download\"\n",
    "        source_path.mkdir(parents=True, exist_ok=True)\n",
    "        target_data_path = path / \"tables\"\n",
    "        target_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        target_metadata_path = path / \"metadata\"\n",
    "        target_metadata_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file in source_path.glob(\"*.zip\"):\n",
    "            with ZipFile(file, \"r\") as zip:\n",
    "                fname_data_lst = [\n",
    "                    fname for fname in zip.namelist() if \"produkt_klima_tag\" in fname\n",
    "                ]\n",
    "                fname_metadata_lst = [\n",
    "                    fname for fname in zip.namelist() if \"Metadaten\" in fname\n",
    "                ]\n",
    "                for fname in fname_data_lst:\n",
    "                    zip.extract(fname, target_path)\n",
    "                for fname in fname_metadata_lst:\n",
    "                    zip.extract(fname, target_path)\n",
    "\n",
    "        files_to_rmv = target_metadata_path.glob(\"*.html\")\n",
    "        for file in files_to_rmv:\n",
    "            file.unlink()\n",
    "\n",
    "        print(\"finished unzipping to folder {}\".format(path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes= {\n",
    "    'STATIONS_ID': \"string\", \n",
    "    'QN_3': \"UInt8\", \n",
    "    'QN_4': \"UInt8\", \n",
    "    ' RSK': \"Float32\", \n",
    "    'RSKF': \"UInt8\", \n",
    "    'SHK_TAG': \"UInt8\",\n",
    "    '  NM': \"string\", \n",
    "    ' TMK': \"Float32\", \n",
    "    ' UPM': \"Float32\", \n",
    "    ' TXK': \"Float32\", \n",
    "    ' TNK': \"Float32\", \n",
    "    ' TGK': \"Float32\",\n",
    "}\n",
    "\n",
    "\n",
    "df_new_lst = [pd.read_table(\n",
    "        file, \n",
    "        sep=\";\",\n",
    "        usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "        'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "        ' TXK', ' TNK', ' TGK'],\n",
    "        dtype_backend=\"pyarrow\",\n",
    "        dtype=dtypes,\n",
    "        na_values=None,\n",
    "    ) for file in Path(\"./data/recent_data/extracts\").glob(\"*.txt\")]\n",
    "\n",
    "df_hist_lst = [pd.read_table(\n",
    "        file, \n",
    "        sep=\";\",\n",
    "        usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "        'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "        ' TXK', ' TNK', ' TGK'],\n",
    "        dtype_backend=\"pyarrow\",\n",
    "        dtype=dtypes,\n",
    "        na_values=None,\n",
    "    ) for file in Path(\"./data/hist_data/extracts\").glob(\"*.txt\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "\n",
    "# df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\", nonexistent=\"NaT\")\n",
    "# df.columns = df.columns.str.replace(' ', '')\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17325569 entries, 0 to 17325568\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Dtype                                     \n",
      "---  ------       -----                                     \n",
      " 0   stations_id  string[pyarrow]                           \n",
      " 1   mess_datum   timestamp[us, tz=Europe/Brussels][pyarrow]\n",
      " 2   qn_3         uint8[pyarrow]                            \n",
      " 3   qn_4         uint8[pyarrow]                            \n",
      " 4   rsk          float[pyarrow]                            \n",
      " 5   rskf         uint8[pyarrow]                            \n",
      " 6   shk_tag      uint8[pyarrow]                            \n",
      " 7   nm           string[pyarrow]                           \n",
      " 8   tmk          float[pyarrow]                            \n",
      " 9   upm          float[pyarrow]                            \n",
      " 10  txk          float[pyarrow]                            \n",
      " 11  tnk          float[pyarrow]                            \n",
      " 12  tgk          float[pyarrow]                            \n",
      "dtypes: float[pyarrow](6), string[pyarrow](2), timestamp[us, tz=Europe/Brussels][pyarrow](1), uint8[pyarrow](4)\n",
      "memory usage: 1020.2 MB\n"
     ]
    }
   ],
   "source": [
    "# df.to_parquet(\"./data/parquet/data.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/main.parquet\", dtype_backend=\"pyarrow\")\n",
    "df.info()\n",
    "\n",
    "df[\"year_month\"] = df[\"mess_datum\"].dt.strftime(\"%Y-%m\")\n",
    "df.to_parquet(\"./data/parquet/main/\", partition_cols=[\"year_month\"], compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes = {\n",
    "#     'Stations_id': \"string\",\n",
    "#     'Stationshoehe': \"Float32\",\n",
    "#     'Geogr.Breite': \"Float32\",\n",
    "#     'Geogr.Laenge': \"Float32\",\n",
    "#     'von_datum': \"string\",\n",
    "#     'bis_datum': \"string\",\n",
    "#     'Stationsname': \"string\",\n",
    "# }\n",
    "\n",
    "# df_new_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype=dtypes,\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "# df_hist_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype=dtypes,\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/hist_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "# df.Stations_id = df.Stations_id.str.replace(' ', '')\n",
    "# df = df.replace(-999, None).astype(dtypes)\n",
    "# df[\"von_datum\"] = pd.to_datetime(df[\"von_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df[\"bis_datum\"] = pd.to_datetime(df[\"bis_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df.columns = df.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_geo.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/metadata_geo.parquet\", dtype_backend=\"pyarrow\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_lst = []\n",
    "for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Stationsname_Betreibername*\"):\n",
    "    with open(file, \"r\", encoding=\"latin1\") as txt_file:\n",
    "                    text = txt_file.read()\n",
    "                    table_with_footer = text.split(\"\\n\\nStations_ID;Betreibername;Von_Datum;Bis_Datum\\n \")[1]\n",
    "                    df_new_lst += [line.split(\";\") for line in table_with_footer.splitlines()[:-1]]\n",
    "\n",
    "df = pd.DataFrame(df_new_lst, columns=[\"stations_id\", \"betreibername\", \"betrieb_von_datum\", \"betrieb_bis_datum\"])    \n",
    "df.stations_id = df.stations_id.str.replace(' ', '')\n",
    "df[\"betrieb_von_datum\"] = pd.to_datetime(df[\"betrieb_von_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "df[\"betrieb_bis_datum\"] = pd.to_datetime(df[\"betrieb_bis_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_operator.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/metadata_operator.parquet\", dtype_backend=\"pyarrow\")\n",
    "# df.info()\n",
    "test = [\"a\", \"b\", \"c\"]\n",
    "if \"a\" in test:\n",
    "    print(\"TRUE\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.types as T\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# spark.conf.set('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'CORRECTED')\n",
    "\n",
    "# schema = T.StructType([\n",
    "#     T.StructField('STATIONS_ID', T.StringType(), False),\n",
    "#     T.StructField('MESS_DATUM', T.TimestampType(), True),\n",
    "#     T.StructField('QN_3', T.ShortType(), True),\n",
    "#     T.StructField('QN_4', T.ShortType(), True),\n",
    "#     T.StructField('RSK', T.DoubleType(), True),\n",
    "#     T.StructField('RSKF', T.ShortType(), True),\n",
    "#     T.StructField('SHK_TAG', T.IntegerType(), True),\n",
    "#     T.StructField('NM', T.StringType(), True),\n",
    "#     T.StructField('TMK', T.DoubleType(), True),\n",
    "#     T.StructField('UPM', T.DoubleType(), True),\n",
    "#     T.StructField('TXK', T.DoubleType(), True),\n",
    "#     T.StructField('TNK', T.DoubleType(), True),\n",
    "#     T.StructField('TGK', T.DoubleType(), True),\n",
    "# ])\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName('SparkNotebook') \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path(\"./data/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .parquet(str(data_path))\n",
    "\n",
    "# df.createOrReplaceTempView(\"temps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT distinct(extract(year from MESS_DATUM)) from temps\n",
    "# where date(MESS_DATUM) >= \"1900-01-01\"\n",
    "# order by 1 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT STATIONS_ID,extract(year from MESS_DATUM), max(TXK) as max_temp_2m from temps\n",
    "# group by 1,2\n",
    "# order by 3 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week7--DKIWNv2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fb13844f58101a74e10bb1f8ffe6c9a5d7ea85b5210f5bc43be278327a5ddab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
